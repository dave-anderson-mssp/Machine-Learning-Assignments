---
title: "679 Assignment 1"
author: "Dave Anderson"
date: "January 31, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


3.1, 3.2, 3.5, 3.6, ,3.11, 3.12, 3.13, 3.14

##3.1)
*The coefficients show the relationship that three variables (TV, radio, and newspaper advertising) have with sales. The null hypothesis for each is that there is no relationship between that individual variable and the response variable of sales, or, in other words, that the variable's corresponding beta coefficient is zero. For TV and Radio, the p value is very small. This indicates that the relationship between each of these variables and sales is not likely to appear just by chance. Newspaper advertising's coefficient has a very high standard error compared to the coefficient, leading to a large p-value, which tells us that newspaper advertising has a weak relationship with sales.*

##3.2)
*Both KNN Classification and KNN Regression identify a neighborhood of the sample space closely related to our* $x_0$ *prediction. KNN is typically used for classification problems and calculates a probability that our prediction point falls within a certain class. Regression is utilized for quantitative responses and creates a function to represent the neighborhood and make a prediction.*

##3.5)

##3.6)
*Our linear regression takes the form:*

$$y = \hat{\beta}_0+\hat{\beta}_1 x$$

*By definition:* 

$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$

*Therefore if we use that and replace x with* $\bar{x}$ *we can conclude:*

$$y = \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 \bar{x}$$

$$y = \bar{y}$$




##3.11)
###a)
```{r}
set.seed(1)
x=rnorm(100)
y=2*x+rnorm (100)

lm1 <- lm(y ~ x + 0)
summary(lm1)
```
*Our coefficient estimate is very close to two, which we should expect because a coefficient of two was used to generate our x and y values. The standard error and p values are both very small, which indicates the relationship of y being twice as large as x is not occuring by chance alone.*

###b)
```{r}
lm2 <- lm(x ~ y +0)
summary(lm2)
```

*Again we have a small p value and standard error, allowing us to reject the null hypothesis that there is no relationship between x and y. According to our coefficient, each unit increase in y leads to a .4 increase in x*

###c)
*Both models had the same t value and very small p values*

###d)

*Numerically:*
```{r}
n <- length(x)
t <- sqrt(n - 1)*(x %*% y)/sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)
t
```

###e)
$x_i y_i$ and $x_j y_j$ are always being multiplied in our formula. if you replace $x_i$ with $y_i$, the formula would produce the same results because of associative property. 

###f)
```{r}
lm3 <- lm(y ~ x)
summary(lm3)

lm4 <- lm(x ~ y)
summary(lm4)


```

##3.12)
###a)

$$\hat{\beta} = \sum_i x_i y_i / \sum_j x_j^2$$

*for regression of y onto x. With regression of x onto y, the only real difference is the x changes to a y on the bottom of the fraction. Therefore, the coefficients are the same if and only if:*

$\sum_j x_j^2 = \sum_j y_j^2$



###b)

```{r}
set.seed(1)

x <- 1:100

y <- 3 * x + rnorm(100)

lmx <- lm(y ~ x + 0)
lmy <- lm(x ~ y + 0)

summary(lmx)
summary(lmy)
```

###c)
```{r}
x <- 1:100
y <- 1:100

lmx2 <- lm(y ~ x + 0)
lmy2 <- lm(x ~ y + 0)

summary(lmx2)
summary(lmy2)
```


##13)
###a)
```{r}
set.seed(1)
x <- rnorm(100)
```

###b)
```{r}
eps <- rnorm(100, sd = sqrt(0.25))
```

###c)
```{r}
y <- -1 + 0.5 * x + eps
```
*The length of y is 100, same as x, and* $\beta_0 = -1  \beta_1 = 0.5$

###d)
```{r}
plot(x,y)
```
*The relationship between x and y is linear, as we would expect. It is not a perfect fit because of our generated noise from adding the error. The residuals would be close to normally distributed, since our added error was created from a normal distribution*
